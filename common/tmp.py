import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_moons
import torch
import torch.nn as nn

#TODO: Two moons

# X, y = make_moons(n_samples=5000, noise=0.1)
# df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))
# colors = {0:'red', 1:'blue'}
# fig, ax = plt.subplots()
# grouped = df.groupby('label')
# for key, group in grouped:
#     group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])
# plt.show()

#TODO: hook in bp，https://www.cnblogs.com/sddai/p/14412250.html
# def hook_fn(grad):
#     print(grad)
#
# x = torch.Tensor([0, 1, 2, 3]).requires_grad_()
# y = torch.Tensor([4, 5, 6, 7]).requires_grad_()
# w = torch.Tensor([1, 2, 3, 4]).requires_grad_()
# z = x+y
# #TODO: tensor hook
# z.register_hook(hook_fn)#backward()才会调用
# z.register_hook(lambda x: 2*x)
# z.register_hook(lambda x: print(x))
# o = w.matmul(z)
#
# print('=====Start backprop=====')
# o.backward()
# print('=====End backprop=====')
#
# print('x.grad:', x.grad)
# print('y.grad:', y.grad)
# print('w.grad:', w.grad)
# print('z.grad:', z.grad)

#TODO: simple module hook, 对复杂模块用了 backward hook，只能得到该模块最后一次简单操作的梯度信息

# class Model(nn.Module):
#     def __init__(self):
#         super(Model, self).__init__()
#         self.fc1 = nn.Linear(3, 4)
#         self.relu1 = nn.ReLU()
#         self.fc2 = nn.Linear(4, 1)
#         self.initialize()
#
#     def initialize(self):
#         with torch.no_grad():
#             self.fc1.weight = torch.nn.Parameter(
#                 torch.Tensor([[1., 2., 3.],
#                               [-4., -5., -6.],
#                               [7., 8., 9.],
#                               [-10., -11., -12.]]))
#
#             self.fc1.bias = torch.nn.Parameter(torch.Tensor([1.0, 2.0, 3.0, 4.0]))
#             self.fc2.weight = torch.nn.Parameter(torch.Tensor([[1.0, 2.0, 3.0, 4.0]]))
#             self.fc2.bias = torch.nn.Parameter(torch.Tensor([1.0]))
#
#     def forward(self, x):
#         o = self.fc1(x)
#         o = self.relu1(o)
#         o = self.fc2(o)
#         return o
#
# # 全局变量，用于存储中间层的 feature
# total_feat_out = []
# total_feat_in = []
# # 定义 forward hook function
# def hook_fn_forward(module, input, output):#forward()才会调用
#     print(module) # 用于区分模块
#     print('input', input) # 首先打印出来
#     print('output', output)
#     total_feat_out.append(output) # 然后分别存入全局 list 中
#     total_feat_in.append(input)
#
# total_grad_out = []
# total_grad_in = []
# def hook_fn_backward(module, grad_input, grad_output):
#     print(module) # 为了区分模块
#     # 为了符合反向传播的顺序，我们先打印 grad_output
#     print('grad_output', grad_output)
#     # 再打印 grad_input
#     print('grad_input', grad_input)
#     # 保存到全局变量
#     total_grad_in.append(grad_input)
#     total_grad_out.append(grad_output)
#
# model = Model()
# #TODO: 对复杂模块用了 backward hook
# #model.register_backward_hook(hook_fn_backward)
# modules = model.named_children() #
# for name, module in modules:
#     module.register_forward_hook(hook_fn_forward)
#     module.register_backward_hook(hook_fn_backward)
#
# x = torch.Tensor([[1.0, 1.0, 1.0]]).requires_grad_()
# o = model(x)
# o.backward()
#
# print('==========Saved inputs and outputs==========')
# for idx in range(len(total_feat_in)):
#     print('input: ', total_feat_in[idx])
#     print('output: ', total_feat_out[idx])
# for idx in range(len(total_grad_in)):
#     print('grad output: ', total_grad_out[idx])
#     print('grad input: ', total_grad_in[idx])

#TODO: visualize
#class Guided_backprop():
#     def __init__(self, model):
#         self.model = model
#         self.image_reconstruction = None
#         self.activation_maps = []
#         self.model.eval()
#         self.register_hooks()
#
#     def register_hooks(self):
#         def first_layer_hook_fn(module, grad_in, grad_out):
#             # 在全局变量中保存输入图片的梯度，该梯度由第一层卷积层
#             # 反向传播得到，因此该函数需绑定第一个 Conv2d Layer
#             self.image_reconstruction = grad_in[0]
#
#         def forward_hook_fn(module, input, output):
#             # 在全局变量中保存 ReLU 层的前向传播输出
#             # 用于将来做 guided backpropagation
#             self.activation_maps.append(output)
#
#         def backward_hook_fn(module, grad_in, grad_out):
#             # ReLU 层反向传播时，用其正向传播的输出作为 guide
#             # 反向传播和正向传播相反，先从后面传起
#             grad = self.activation_maps.pop()
#             # ReLU 正向传播的输出要么大于0，要么等于0，
#             # 大于 0 的部分，梯度为1，
#             # 等于0的部分，梯度还是 0
#             grad[grad > 0] = 1
#
#             # grad_in[0] 表示 feature 的梯度，只保留大于 0 的部分
#             positive_grad_in = torch.clamp(grad_in[0], min=0.0)
#             # 创建新的输入端梯度
#             new_grad_in = positive_grad_in * grad
#
#             # ReLU 不含 parameter，输入端梯度是一个只有一个元素的 tuple
#             return (new_grad_in,)
#
#         # 获取 module，这里只针对 alexnet，如果是别的，则需修改
#         modules = list(self.model.features.named_children())
#
#         # 遍历所有 module，对 ReLU 注册 forward hook 和 backward hook
#         for name, module in modules:
#             if isinstance(module, nn.ReLU):
#                 module.register_forward_hook(forward_hook_fn)
#                 module.register_backward_hook(backward_hook_fn)
#
#         # 对第1层卷积层注册 hook
#         first_layer = modules[0][1]
#         first_layer.register_backward_hook(first_layer_hook_fn)
#
#     def visualize(self, input_image, target_class):
#         # 获取输出，之前注册的 forward hook 开始起作用
#         model_output = self.model(input_image)
#         self.model.zero_grad()
#         pred_class = model_output.argmax().item()
#
#         # 生成目标类 one-hot 向量，作为反向传播的起点
#         grad_target_map = torch.zeros(model_output.shape,
#                                       dtype=torch.float)
#         if target_class is not None:
#             grad_target_map[0][target_class] = 1
#         else:
#             grad_target_map[0][pred_class] = 1
#
#         # 反向传播，之前注册的 backward hook 开始起作用
#         model_output.backward(grad_target_map)
#         # 得到 target class 对输入图片的梯度，转换成图片格式
#         result = self.image_reconstruction.data[0].permute(1, 2, 0)
#         return result.numpy()
#
#
# def normalize(I):
#     # 归一化梯度map，先归一化到 mean=0 std=1
#     norm = (I - I.mean()) / I.std()
#     # 把 std 重置为 0.1，让梯度map中的数值尽可能接近 0
#     norm = norm * 0.1
#     # 均值加 0.5，保证大部分的梯度值为正
#     norm = norm + 0.5
#     # 把 0，1 以外的梯度值分别设置为 0 和 1
#     norm = norm.clip(0, 1)
#     return norm
#
#
# if __name__ == '__main__':
#     from torchvision import models, transforms
#     from PIL import Image
#     import matplotlib.pyplot as plt
#
#     image_path = '{}/SLN-master/data/cat.png'.format('/users6/ttwu/script/Robustness')#/home/zhh/code
#     I = Image.open(image_path).convert('RGB')
#     means = [0.485, 0.456, 0.406]
#     stds = [0.229, 0.224, 0.225]
#     size = 224
#
#     transform = transforms.Compose([
#         transforms.Resize(size),
#         transforms.CenterCrop(size),
#         transforms.ToTensor(),
#         transforms.Normalize(means, stds)
#     ])
#
#     tensor = transform(I).unsqueeze(0).requires_grad_()
#
#     model = models.alexnet(pretrained=True)
#
#     guided_bp = Guided_backprop(model)
#     result = guided_bp.visualize(tensor, None)
#
#     result = normalize(result)
#     plt.imshow(result)
#     plt.show()
#
#     print('END')

#TODO: https://github.com/cpury/keras_gradient_noise，在所有的参数上加扰动
#gaussian noise on the gradient of loss w.r.t parameters (Neelakantan et al., 2015)
# def get_gradients(self, loss, params):
#     grads = super(NoisyOptimizer, self).get_gradients(loss, params)
#
#     # Add decayed gaussian noise
#     t = K.cast(self.iterations, K.dtype(grads[0]))
#     variance = self.noise_eta / ((1 + t) ** self.noise_gamma)
#
#     grads = [
#         grad + K.random_normal(
#             _get_shape(grad),
#             mean=0.0,
#             stddev=K.sqrt(variance),
#             dtype=K.dtype(grads[0])
#         )
#         for grad in grads
#     ]
#
#     return grads

#TODO: pytorch bug on register_backward_hook
# https://oldpan.me/archives/pytorch-autograd-hook
#device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#
# class MyMul(nn.Module):
#     def forward(self, input):
#         out = input * 2
#         return out
#
# class MyMean(nn.Module):            # 自定义除法module
#     def forward(self, input):
#         out = input/4
#         return out
#
# def tensor_hook(grad):
#     print('tensor hook')
#     print('grad:', grad)
#     return grad
#
# class MyNet(nn.Module):
#     def __init__(self):
#         super(MyNet, self).__init__()
#         self.f1 = nn.Linear(4, 1, bias=True)
#         self.f2 = MyMean()
#         self.weight_init()
#
#     def forward(self, input):
#         #TODO: pytorch bug
#         #相同输入，相同layers,调用顺序不同，输出result不同(正常现象)，
#         #但register_backward_hook只能操作简单模块，而不能操作包含多个子模块的复杂模块
#         #如果对复杂模块用了 backward hook，e.g.对MyNet整体进行register_backward_hook，那么我们只能得到该模块最后一次简单操作的梯度信息，即f1或f2,因此对应的输出original grad也不同
#         self.input = input
#         output = self.f1(input)       # 先进行运算1，后进行运算2
#         output = self.f2(output)
#         # output = self.f2(input)       # 先进行运算2，后进行运算1
#         # output = self.f1(output)
#         return output
#
#     def weight_init(self):
#         self.f1.weight.data.fill_(8.0)    # 这里设置Linear的权重为8
#         self.f1.bias.data.fill_(2.0)      # 这里设置Linear的bias为2
#
#     def my_hook(self, module, grad_input, grad_output):
#         print('doing my_hook')
#         print('original grad:', grad_input)
#         print('original outgrad:', grad_output)
#         # grad_input = grad_input[0]*self.input   # 这里把hook函数内对grad_input的操作进行了注释，
#         # grad_input = tuple([grad_input])        # 返回的grad_input必须是tuple，所以我们进行了tuple包装。
#         # print('now grad:', grad_input)
#
#         return grad_input
#
# if __name__ == '__main__':
#
#     input = torch.tensor([1, 2, 3, 4], dtype=torch.float32, requires_grad=True).to(device)
#
#     net = MyNet()
#     net.to(device)
#
#     net.register_backward_hook(net.my_hook)   # 这两个hook函数一定要result = net(input)执行前执行，因为hook函数实在forward的时候进行绑定的
#     input.register_hook(tensor_hook)
#     result = net(input)
#
#     print('result =', result)
#
#     result.backward()
#
#     print('input.grad:', input.grad)
#     for param in net.parameters():
#         print('{}:grad->{}'.format(param, param.grad))

#TODO: loss using cosine function clipping
import numpy as np
import matplotlib.pyplot as plt
#loss.max()=2.7751,loss.min()=1.8815
# loss = [2.2119e+00, 1.3224e-01, 1.6835e-01, 5.2355e-02, 3.6499e-01, 3.5858e-02,
#         8.9062e-01, 2.7006e-02, 1.3149e+00, 5.9880e-02, 2.3468e-01, 5.0837e-02,
#         1.3766e+00, 2.6169e-02, 2.9340e-01, 6.0261e-01, 1.8415e+00, 5.1255e-01,
#         3.6366e-01, 4.1199e-01, 3.8772e-01, 1.7325e-01, 4.0306e+00, 1.4522e-01,
#         9.3550e-01, 1.2262e+00, 6.9791e-01, 1.0373e-01, 2.4136e-01, 5.6570e-02,
#         5.7953e-01, 3.7847e+00, 5.6911e-01, 1.6984e-02, 1.1329e-02, 8.6127e-02,
#         1.0127e-01, 4.2128e+00, 2.0632e+00, 2.9318e-02, 1.7556e+00, 9.1133e-01,
#         1.1041e+00, 2.6794e-02, 3.5094e-01, 1.0341e+00, 7.4055e-01, 2.4518e-01,
#         1.3822e+00, 8.0900e-03, 1.4016e+00, 1.3058e+00, 9.5682e-02, 2.4553e-01,
#         1.5357e-01, 1.1908e-01, 1.2465e-03, 1.3292e+00, 7.4923e-01, 6.3117e-02,
#         8.3807e-03, 1.8450e+00, 2.8974e+00, 2.9197e-02, 3.7700e-01, 1.3417e-01,
#         8.3998e-01, 4.6756e-02, 2.0435e-01, 1.2069e-02, 2.6520e+00, 1.9958e+00,
#         4.3836e-02, 6.2976e-01, 8.6477e-01, 1.7105e-01, 2.0316e+00, 1.4635e+00,
#         7.0305e-01, 1.6258e-01, 2.1862e+00, 2.0913e-01, 1.8003e-01, 2.3182e-01,
#         1.1994e+00, 1.6309e+00, 2.8734e-02, 1.3257e+00, 1.3346e-01, 1.6325e+00,
#         7.7300e-03, 3.5188e+00, 1.9984e-01, 4.0621e-01, 2.6781e-03, 5.0527e-01,
#         8.5156e-01, 2.4769e+00, 3.0979e-01, 2.6439e-01, 1.1190e-01, 3.0374e+00,
#         1.8304e-01, 3.0550e+00, 1.4179e-01, 1.2361e+00, 8.8710e-01, 1.1345e-01,
#         1.1235e-01, 1.4931e+00, 7.8796e-02, 3.1576e-01, 2.5233e-02, 1.5218e-01,
#         4.7243e-02, 1.6865e-01, 5.0536e-02, 3.2611e+00, 4.5117e-01, 7.5850e-01,
#         2.6362e-02, 3.3704e-01, 2.1425e+00, 3.1884e+00, 6.3902e-02, 8.7856e-01,
#         1.0601e+00, 1.8948e-02]
        # [2.0646e-03, 2.9277e-01, 3.9174e-01, 1.0126e-01, 3.8002e-02, 2.1453e-03,
        # 1.7512e-02, 4.4935e-02, 1.5881e-01, 1.2275e-02, 9.4779e-02, 8.1746e-02,
        # 1.7959e-02, 1.1798e+00, 2.0197e+00, 4.8661e-04, 3.2630e-01, 4.0109e-01,
        # 1.4655e-01, 8.4122e-01, 6.2056e-02, 3.9286e-01, 5.0524e-03, 1.6742e-02,
        # 3.6911e-01, 1.6270e+00, 4.1497e-02, 1.0607e-01, 1.2057e+00, 1.3794e-01,
        # 1.2151e+00, 1.3241e-02, 5.2361e-02, 1.7558e-02, 1.5768e-02, 1.7408e+00,
        # 1.0176e+00, 9.4973e-01, 6.5707e-01, 2.5655e-02, 1.0163e-02, 1.6064e-01,
        # 1.3077e-01, 1.7948e-02, 6.8639e-02, 3.2494e+00, 2.4418e-01, 3.3251e+00,
        # 3.8230e-01, 2.3943e-03, 1.4292e-01, 2.7384e-02, 9.4922e-02, 1.0017e-01,
        # 7.4199e-03, 4.7290e-01, 7.4182e-02, 3.4861e-01, 8.1216e-01, 1.7233e+00,
        # 3.2217e-03, 2.8426e-01, 3.3694e-03, 1.5578e-02, 2.3816e+00, 1.6270e-02,
        # 8.4067e-01, 1.4841e+00, 1.0733e-01, 1.2183e-02, 8.0320e-01, 5.0789e-01,
        # 5.3712e-01, 7.7813e-02, 1.5942e-01, 5.6974e-02, 8.8693e-02, 1.0251e+00,
        # 6.4391e-03, 5.5439e-03, 3.0812e-01, 2.9976e-01, 3.2538e-03, 1.6465e+00,
        # 4.0053e-02, 3.4196e-01, 4.6312e-03, 1.5483e-01, 2.1454e-01, 5.2935e-01,
        # 6.2172e-02, 2.4425e-02, 4.8629e-01, 6.6735e-02, 1.1604e-01, 1.0869e-01,
        # 7.7642e-01, 1.7441e+00, 4.1057e-01, 8.4641e-03, 5.1387e-02, 6.1091e-02,
        # 8.3341e-01, 8.4662e-04, 1.0414e+00, 6.9214e-03, 3.6661e-01, 1.8143e-02,
        # 1.0762e+00, 2.2328e+00, 3.7042e-01, 2.8322e-01, 9.9410e-01, 3.2814e-02,
        # 1.9599e-02, 2.0749e-01, 1.1014e-02, 8.5118e-02, 2.0973e-01, 3.4350e-01,
        # 8.5687e-01, 8.1037e-01, 3.4969e-02, 1.1833e+00, 2.1098e+00, 2.3163e-01,
        # 1.5664e-01, 2.4556e-01]
        # [2.0378, 2.5707, 2.3375, 2.4994, 2.5390, 2.1753, 2.4838, 2.2473, 2.7030,
        # 2.0123, 2.3010, 2.3819, 2.2188, 2.0857, 2.4322, 2.4905, 2.4505, 2.3754,
        # 2.4760, 2.3606, 2.5217, 2.2050, 2.3821, 2.6055, 2.5353, 2.3934, 2.3145,
        # 2.1862, 2.4883, 2.3873, 2.3506, 2.1594, 2.4764, 2.0468, 2.5094, 2.5241,
        # 2.5418, 2.1811, 2.4510, 2.4348, 2.2644, 2.4702, 2.2062, 2.0632, 2.3682,
        # 2.5211, 2.4398, 2.2703, 2.1821, 2.3667, 1.9429, 2.4290, 2.4254, 2.1359,
        # 2.2072, 2.3743, 2.1939, 2.7751, 2.4144, 2.2537, 2.2558, 2.4191, 2.3688,
        # 2.3270, 2.3841, 2.6446, 2.2366, 2.0242, 2.3084, 2.4057, 2.3969, 1.9503,
        # 2.2424, 2.4315, 2.0357, 2.5029, 2.4118, 2.1593, 2.4704, 2.4111, 2.2751,
        # 2.2213, 2.5312, 2.4047, 2.1597, 2.5132, 2.5216, 2.5848, 2.3800, 2.0335,
        # 2.4013, 2.4762, 2.1216, 2.3341, 2.2074, 1.9063, 2.3134, 2.5897, 2.6655,
        # 2.3162, 2.2726, 2.3601, 2.4089, 2.0614, 2.1303, 2.6538, 2.2934, 2.1673,
        # 2.4853, 2.5429, 2.2747, 2.2443, 2.3159, 2.2425, 2.3469, 2.2339, 2.0546,
        # 2.0883, 2.3194, 2.4901, 2.3650, 2.3598, 2.1994, 2.2759, 2.3918, 1.8815,
        # 2.5153, 2.1719]
# loss_norm= [5.2490e-01, 3.1104e-02, 3.9676e-02, 1.2135e-02, 8.6369e-02, 8.2182e-03,
#         2.1117e-01, 6.1163e-03, 3.1192e-01, 1.3922e-02, 5.5428e-02, 1.1775e-02,
#         3.2657e-01, 5.9177e-03, 6.9369e-02, 1.4279e-01, 4.3694e-01, 1.2141e-01,
#         8.6052e-02, 9.7528e-02, 9.1765e-02, 4.0842e-02, 9.5673e-01, 3.4184e-02,
#         2.2183e-01, 2.9086e-01, 1.6542e-01, 2.4335e-02, 5.7013e-02, 1.3136e-02,
#         1.3731e-01, 8.9835e-01, 1.3483e-01, 3.7368e-03, 2.3940e-03, 2.0154e-02,
#         2.3750e-02, 1.0000e+00, 4.8960e-01, 6.6653e-03, 4.1656e-01, 2.1609e-01,
#         2.6186e-01, 6.0659e-03, 8.3032e-02, 2.4525e-01, 1.7554e-01, 5.7919e-02,
#         3.2790e-01, 1.6249e-03, 3.3250e-01, 3.0974e-01, 2.2423e-02, 5.8002e-02,
#         3.6169e-02, 2.7979e-02, 0.0000e+00, 3.1530e-01, 1.7760e-01, 1.4691e-02,
#         1.6940e-03, 4.3778e-01, 6.8766e-01, 6.6366e-03, 8.9219e-02, 3.1561e-02,
#         1.9915e-01, 1.0806e-02, 4.8226e-02, 2.5696e-03, 6.2940e-01, 4.7360e-01,
#         1.0112e-02, 1.4924e-01, 2.0504e-01, 4.0318e-02, 4.8208e-01, 3.4720e-01,
#         1.6664e-01, 3.8308e-02, 5.1879e-01, 4.9360e-02, 4.2450e-02, 5.4748e-02,
#         2.8449e-01, 3.8695e-01, 6.5266e-03, 3.1448e-01, 3.1393e-02, 3.8733e-01,
#         1.5394e-03, 8.3520e-01, 4.7155e-02, 9.6154e-02, 3.3993e-04, 1.1968e-01,
#         2.0190e-01, 5.8782e-01, 7.3262e-02, 6.2481e-02, 2.6274e-02, 7.2091e-01,
#         4.3165e-02, 7.2508e-01, 3.3372e-02, 2.9321e-01, 2.1034e-01, 2.6642e-02,
#         2.6381e-02, 3.5423e-01, 1.8413e-02, 7.4679e-02, 5.6953e-03, 3.5838e-02,
#         1.0921e-02, 3.9749e-02, 1.1703e-02, 7.7402e-01, 1.0683e-01, 1.7980e-01,
#         5.9634e-03, 7.9730e-02, 5.0843e-01, 7.5677e-01, 1.4877e-02, 2.0831e-01,
#         2.5141e-01, 4.2031e-03]
        # [4.7464e-04, 8.7916e-02, 1.1768e-01, 3.0311e-02, 1.1284e-02, 4.9890e-04,
        # 5.1210e-03, 1.3370e-02, 4.7622e-02, 3.5458e-03, 2.8362e-02, 2.4442e-02,
        # 5.2556e-03, 3.5471e-01, 6.0735e-01, 0.0000e+00, 9.8002e-02, 1.2050e-01,
        # 4.3936e-02, 2.5288e-01, 1.8519e-02, 1.1802e-01, 1.3733e-03, 4.8895e-03,
        # 1.1088e-01, 4.8923e-01, 1.2336e-02, 3.1757e-02, 3.6251e-01, 4.1346e-02,
        # 3.6533e-01, 3.8364e-03, 1.5603e-02, 5.1347e-03, 4.5965e-03, 5.2348e-01,
        # 3.0592e-01, 2.8552e-01, 1.9749e-01, 7.5705e-03, 2.9106e-03, 4.8171e-02,
        # 3.9187e-02, 5.2521e-03, 2.0499e-02, 9.7724e-01, 7.3299e-02, 1.0000e+00,
        # 1.1485e-01, 5.7381e-04, 4.2844e-02, 8.0904e-03, 2.8405e-02, 2.9984e-02,
        # 2.0854e-03, 1.4210e-01, 2.2167e-02, 1.0471e-01, 2.4414e-01, 5.1819e-01,
        # 8.2268e-04, 8.5355e-02, 8.6710e-04, 4.5392e-03, 7.1621e-01, 4.7475e-03,
        # 2.5272e-01, 4.4625e-01, 3.2137e-02, 3.5180e-03, 2.4145e-01, 1.5262e-01,
        # 1.6141e-01, 2.3259e-02, 4.7804e-02, 1.6991e-02, 2.6532e-02, 3.0819e-01,
        # 1.7904e-03, 1.5212e-03, 9.2533e-02, 9.0017e-02, 8.3233e-04, 4.9509e-01,
        # 1.1901e-02, 1.0271e-01, 1.2466e-03, 4.6426e-02, 6.4385e-02, 1.5908e-01,
        # 1.8554e-02, 7.2004e-03, 1.4613e-01, 1.9927e-02, 3.4757e-02, 3.2546e-02,
        # 2.3339e-01, 5.2445e-01, 1.2335e-01, 2.3995e-03, 1.5310e-02, 1.8229e-02,
        # 2.5053e-01, 1.0829e-04, 3.1308e-01, 1.9355e-03, 1.1013e-01, 5.3109e-03,
        # 3.2355e-01, 6.7147e-01, 1.1127e-01, 8.5043e-02, 2.9887e-01, 9.7236e-03,
        # 5.7487e-03, 6.2263e-02, 3.1665e-03, 2.5456e-02, 6.2938e-02, 1.0317e-01,
        # 2.5759e-01, 2.4360e-01, 1.0372e-02, 3.5578e-01, 6.3446e-01, 6.9526e-02,
        # 4.6968e-02, 7.3714e-02]
        # \
        # [0.1749, 0.7712, 0.5102, 0.6914, 0.7358, 0.3287, 0.6740, 0.4094, 0.9193,
        # 0.1464, 0.4694, 0.5600, 0.3774, 0.2284, 0.6162, 0.6815, 0.6367, 0.5527,
        # 0.6652, 0.5361, 0.7165, 0.3620, 0.5602, 0.8101, 0.7317, 0.5728, 0.4845,
        # 0.3410, 0.6790, 0.5660, 0.5249, 0.3109, 0.6657, 0.1849, 0.7027, 0.7191,
        # 0.7389, 0.3352, 0.6373, 0.6192, 0.4285, 0.6588, 0.3634, 0.2033, 0.5446,
        # 0.7158, 0.6247, 0.4351, 0.3364, 0.5429, 0.0687, 0.6126, 0.6086, 0.2846,
        # 0.3644, 0.5514, 0.3495, 1.0000, 0.5963, 0.4165, 0.4188, 0.6016, 0.5453,
        # 0.4985, 0.5624, 0.8539, 0.3974, 0.1597, 0.4777, 0.5866, 0.5767, 0.0769,
        # 0.4038, 0.6155, 0.1725, 0.6953, 0.5934, 0.3109, 0.6589, 0.5926, 0.4405,
        # 0.3802, 0.7270, 0.5855, 0.3113, 0.7069, 0.7163, 0.7870, 0.5578, 0.1700,
        # 0.5816, 0.6655, 0.2686, 0.5065, 0.3647, 0.0277, 0.4833, 0.7925, 0.8774,
        # 0.4864, 0.4377, 0.5355, 0.5901, 0.2013, 0.2784, 0.8643, 0.4609, 0.3198,
        # 0.6757, 0.7401, 0.4400, 0.4060, 0.4861, 0.4040, 0.5208, 0.3943, 0.1937,
        # 0.2313, 0.4900, 0.6811, 0.5411, 0.5352, 0.3557, 0.4414, 0.5710, 0.0000,
        # 0.7093, 0.3249]
#|l-tau|,l_tau.max()=0.4518, l_tau.min()=0.0008
# l_tau = [1.5193, 0.5603, 0.5242, 0.6402, 0.3276, 0.6567, 0.1981, 0.6656, 0.6224,
#         0.6327, 0.4579, 0.6417, 0.6841, 0.6664, 0.3992, 0.0900, 1.1489, 0.1800,
#         0.3289, 0.2806, 0.3048, 0.5193, 3.3380, 0.5473, 0.2429, 0.5337, 0.0053,
#         0.5888, 0.4512, 0.6360, 0.1130, 3.0921, 0.1235, 0.6756, 0.6812, 0.6064,
#         0.5913, 3.5203, 1.3707, 0.6632, 1.0630, 0.2188, 0.4115, 0.6658, 0.3416,
#         0.3416, 0.0480, 0.4474, 0.6897, 0.6845, 0.7091, 0.6132, 0.5969, 0.4470,
#         0.5390, 0.5735, 0.6913, 0.6366, 0.0567, 0.6294, 0.6842, 1.1524, 2.2048,
#         0.6634, 0.3156, 0.5584, 0.1474, 0.6458, 0.4882, 0.6805, 1.9594, 1.3033,
#         0.6487, 0.0628, 0.1722, 0.5215, 1.3390, 0.7709, 0.0105, 0.5300, 1.4936,
#         0.4834, 0.5125, 0.4607, 0.5068, 0.9383, 0.6638, 0.6331, 0.5591, 0.9399,
#         0.6848, 2.8262, 0.4927, 0.2864, 0.6899, 0.1873, 0.1590, 1.7843, 0.3828,
#         0.4282, 0.5807, 2.3448, 0.5095, 2.3624, 0.5508, 0.5436, 0.1945, 0.5791,
#         0.5802, 0.8006, 0.6138, 0.3768, 0.6673, 0.5404, 0.6453, 0.5239, 0.6420,
#         2.5685, 0.2414, 0.0659, 0.6662, 0.3555, 1.4500, 2.4959, 0.6287, 0.1860,
#         0.3675, 0.6736]
        # [0.5133, 0.2226, 0.1236, 0.4141, 0.4774, 0.5132, 0.4979, 0.4704, 0.3566,
        # 0.5031, 0.4206, 0.4336, 0.4974, 0.6644, 1.5043, 0.5149, 0.1891, 0.1143,
        # 0.3688, 0.3258, 0.4533, 0.1225, 0.5103, 0.4986, 0.1463, 1.1116, 0.4739,
        # 0.4093, 0.6903, 0.3774, 0.6997, 0.5021, 0.4630, 0.4978, 0.4996, 1.2255,
        # 0.5022, 0.4344, 0.1417, 0.4897, 0.5052, 0.3547, 0.3846, 0.4974, 0.4467,
        # 2.7340, 0.2712, 2.8097, 0.1331, 0.5130, 0.3724, 0.4880, 0.4204, 0.4152,
        # 0.5080, 0.0425, 0.4412, 0.1668, 0.2968, 1.2079, 0.5121, 0.2311, 0.5120,
        # 0.4998, 1.8662, 0.4991, 0.3253, 0.9687, 0.4080, 0.5032, 0.2878, 0.0075,
        # 0.0218, 0.4376, 0.3560, 0.4584, 0.4267, 0.5097, 0.5089, 0.5098, 0.2072,
        # 0.2156, 0.5121, 1.1311, 0.4753, 0.1734, 0.5107, 0.3605, 0.3008, 0.0140,
        # 0.4532, 0.4909, 0.0291, 0.4486, 0.3993, 0.4067, 0.2610, 1.2287, 0.1048,
        # 0.5069, 0.4640, 0.4543, 0.3180, 0.5145, 0.5260, 0.5084, 0.1488, 0.4972,
        # 0.5608, 1.7175, 0.1450, 0.2322, 0.4787, 0.4826, 0.4958, 0.3079, 0.5044,
        # 0.4303, 0.3056, 0.1719, 0.3415, 0.2950, 0.4804, 0.6680, 1.5945, 0.2837,
        # 0.3587, 0.2698]
        # [0.2955, 0.2373, 0.0041, 0.1660, 0.2057, 0.1581, 0.1505, 0.0860, 0.3696,
        # 0.3210, 0.0324, 0.0486, 0.1145, 0.2477, 0.0988, 0.1572, 0.1171, 0.0421,
        # 0.1426, 0.0272, 0.1884, 0.1283, 0.0487, 0.2721, 0.2020, 0.0600, 0.0189,
        # 0.1471, 0.1549, 0.0540, 0.0173, 0.1740, 0.1430, 0.2866, 0.1761, 0.1908,
        # 0.2084, 0.1522, 0.1177, 0.1015, 0.0689, 0.1369, 0.1271, 0.2702, 0.0348,
        # 0.1878, 0.1064, 0.0630, 0.1512, 0.0333, 0.3904, 0.0956, 0.0920, 0.1975,
        # 0.1262, 0.0409, 0.1395, 0.4418, 0.0811, 0.0796, 0.0775, 0.0858, 0.0355,
        # 0.0063, 0.0508, 0.3112, 0.0967, 0.3091, 0.0249, 0.0724, 0.0635, 0.3831,
        # 0.0910, 0.0982, 0.2976, 0.1695, 0.0784, 0.1740, 0.1370, 0.0777, 0.0582,
        # 0.1120, 0.1978, 0.0713, 0.1737, 0.1798, 0.1883, 0.2514, 0.0467, 0.2999,
        # 0.0679, 0.1429, 0.2118, 0.0008, 0.1259, 0.4271, 0.0199, 0.2564, 0.3322,
        # 0.0171, 0.0607, 0.0267, 0.0755, 0.2720, 0.2030, 0.3205, 0.0400, 0.1660,
        # 0.1520, 0.2095, 0.0586, 0.0890, 0.0174, 0.0908, 0.0136, 0.0995, 0.2787,
        # 0.2451, 0.0139, 0.1568, 0.0317, 0.0264, 0.1340, 0.0574, 0.0584, 0.4518,
        # 0.1820, 0.1615]
#cos(0.5pi*(loss_norm+t/T))
# filter = [ 0.3901,  0.9232,  0.9180,  0.9343,  0.8864,  0.9364,  0.7793,  0.9376,
#          0.6708,  0.9332,  0.9079,  0.9345,  0.6535,  0.9377,  0.8985,  0.8420,
#          0.5132,  0.8596,  0.8867,  0.8782,  0.8825,  0.9172, -0.2740,  0.9213,
#          0.7687,  0.6949,  0.8223,  0.9272,  0.9068,  0.9337,  0.8466, -0.1848,
#          0.8486,  0.9389,  0.9396,  0.9297,  0.9276, -0.3387,  0.4405,  0.9373,
#          0.5404,  0.7744,  0.7270,  0.9376,  0.8888,  0.7446,  0.8131,  0.9062,
#          0.6519,  0.9400,  0.6465,  0.6733,  0.9284,  0.9062,  0.9201,  0.9251,
#          0.9409,  0.6668,  0.8112,  0.9328,  0.9400,  0.5120,  0.1445,  0.9373,
#          0.8843,  0.9229,  0.7910,  0.9350,  0.9125,  0.9395,  0.2344,  0.4629,
#          0.9354,  0.8365,  0.7853,  0.9176,  0.4511,  0.6287,  0.8212,  0.9188,
#          0.3989,  0.9118,  0.9162,  0.9083,  0.7021,  0.5789,  0.9374,  0.6678,
#          0.9230,  0.5784,  0.9401, -0.0866,  0.9132,  0.8792,  0.9407,  0.8610,
#          0.7883,  0.2973,  0.8958,  0.9032,  0.9261,  0.0927,  0.9158,  0.0862,
#          0.9218,  0.6923,  0.7801,  0.9259,  0.9260,  0.6200,  0.9307,  0.8948,
#          0.9378,  0.9203,  0.9349,  0.9179,  0.9345,  0.0094,  0.8711,  0.8092,
#          0.9377,  0.8912,  0.4138,  0.0365,  0.9327,  0.7821,  0.7381,  0.9386]
        # [ 0.7496,  0.6519,  0.6158,  0.7178,  0.7383,  0.7496,  0.7448,  0.7361,
        #  0.6986,  0.7464,  0.7199,  0.7242,  0.7446,  0.2870, -0.1056,  0.7501,
        #  0.6398,  0.6123,  0.7027,  0.4359,  0.7306,  0.6154,  0.7487,  0.7450,
        #  0.6242,  0.0797,  0.7372,  0.7162,  0.2752,  0.7056,  0.2709,  0.7461,
        #  0.7337,  0.7448,  0.7453,  0.0259,  0.3595,  0.3892,  0.5124,  0.7422,
        #  0.7471,  0.6980,  0.7080,  0.7446,  0.7284, -0.6341,  0.6692, -0.6613,
        #  0.6193,  0.7495,  0.7039,  0.7416,  0.7199,  0.7181,  0.7479,  0.5851,
        #  0.7266,  0.6317,  0.4482,  0.0343,  0.7493,  0.6550,  0.7492,  0.7454,
        # -0.2733,  0.7452,  0.4361,  0.1467,  0.7158,  0.7464,  0.4520,  0.5716,
        #  0.5602,  0.7255,  0.6984,  0.7322,  0.7219,  0.3561,  0.7482,  0.7485,
        #  0.6464,  0.6494,  0.7492,  0.0705,  0.7376,  0.6341,  0.7488,  0.6999,
        #  0.6795,  0.5633,  0.7305,  0.7426,  0.5800,  0.7290,  0.7129,  0.7153,
        #  0.4632,  0.0244,  0.6087,  0.7476,  0.7340,  0.7309,  0.4392,  0.7500,
        #  0.3489,  0.7481,  0.6251,  0.7446,  0.3335, -0.2050,  0.6237,  0.6553,
        #  0.3698,  0.7399,  0.7441,  0.6820,  0.7468,  0.7231,  0.6812,  0.6336,
        #  0.4292,  0.4489,  0.7392,  0.2853, -0.1478,  0.6736,  0.6993,  0.6687]
        # [0.9611,  0.3468,  0.6919,  0.4613,  0.3984,  0.8670,  0.4854,  0.7971,
        #  0.1213,  0.9725,  0.7367,  0.6334,  0.8264,  0.9345,  0.5627,  0.4751,
        #  0.5358,  0.6422,  0.4974,  0.6620,  0.4261,  0.8398,  0.6332,  0.2888,
        #  0.4043,  0.6177,  0.7205,  0.8573,  0.4785,  0.6261,  0.6750,  0.8806,
        #  0.4968,  0.9566,  0.4456,  0.4223,  0.3940,  0.8619,  0.5350,  0.5588,
        #  0.7787,  0.5061,  0.8386,  0.9478,  0.6519,  0.4271,  0.5516,  0.7721,
        #  0.8610,  0.6539,  0.9936,  0.5673,  0.5725,  0.8994,  0.8377,  0.6438,
        #  0.8503, -0.0052,  0.5882,  0.7903,  0.7881,  0.5815,  0.6511,  0.7050,
        #  0.6304,  0.2224,  0.8084,  0.9674,  0.7278,  0.6005,  0.6128,  0.9921,
        #  0.8024,  0.5636,  0.9621,  0.4559,  0.5920,  0.8807,  0.5060,  0.5929,
        #  0.7667,  0.8239,  0.4111,  0.6019,  0.8804,  0.4397,  0.4263,  0.3235,
        #  0.6360,  0.9632,  0.6068,  0.4970,  0.9101,  0.6961,  0.8375,  0.9988,
        #  0.7218,  0.3152,  0.1863,  0.7184,  0.7695,  0.6627,  0.5961,  0.9488,
        #  0.9037,  0.2065,  0.7457,  0.8739,  0.4831,  0.3922,  0.7671,  0.8003,
        #  0.7187,  0.8022,  0.6798,  0.8112,  0.9525,  0.9328,  0.7144,  0.4757,
        #  0.6561,  0.6631,  0.8451,  0.7658,  0.6199,  1.0000,  0.4363,  0.8700]
#std_d=filter*l_tau
# std_d = [ 0.1680,  0.1458,  0.1355,  0.1687,  0.0813,  0.1735,  0.0427,  0.1761,
#          0.1177,  0.1666,  0.1169,  0.1692,  0.1262,  0.1764,  0.1007,  0.0203,
#          0.1670,  0.0427,  0.0816,  0.0688,  0.0752,  0.1341, -0.2598,  0.1421,
#          0.0520,  0.1045,  0.0000,  0.1539,  0.1150,  0.1675,  0.0259, -0.1623,
#          0.0285,  0.1790,  0.1807,  0.1590,  0.1546, -0.3387,  0.1711,  0.1754,
#          0.1626,  0.0470,  0.0840,  0.1762,  0.0850,  0.0712,  0.0099,  0.1140,
#          0.1269,  0.1816,  0.1294,  0.1164,  0.1562,  0.1139,  0.1397,  0.1495,
#          0.1836,  0.1198,  0.0118,  0.1656,  0.1815,  0.1671,  0.0904,  0.1755,
#          0.0781,  0.1452,  0.0320,  0.1704,  0.1254,  0.1805,  0.1303,  0.1709,
#          0.1712,  0.0137,  0.0373,  0.1347,  0.1712,  0.1369,  0.0012,  0.1371,
#          0.1689,  0.1240,  0.1322,  0.1177,  0.1002,  0.1537,  0.1756,  0.1193,
#          0.1454,  0.1538,  0.1817, -0.0695,  0.1266,  0.0703,  0.1832,  0.0446,
#          0.0345,  0.1505,  0.0962,  0.1086,  0.1516,  0.0617,  0.1314,  0.0578,
#          0.1430,  0.1060,  0.0420,  0.1511,  0.1515,  0.1403,  0.1611,  0.0946,
#          0.1766,  0.1401,  0.1702,  0.1354,  0.1693,  0.0069,  0.0585,  0.0139,
#          0.1763,  0.0888,  0.1701,  0.0259,  0.1654,  0.0402,  0.0761,  0.1785]
        # [ 0.1353,  0.0500,  0.0255,  0.1042,  0.1238,  0.1353,  0.1303,  0.1216,
        #  0.0870,  0.1320,  0.1061,  0.1101,  0.1302,  0.0673, -0.0564,  0.1358,
        #  0.0415,  0.0233,  0.0906,  0.0495,  0.1162,  0.0253,  0.1343,  0.1306,
        #  0.0309,  0.0314,  0.1227,  0.1027,  0.0671,  0.0932,  0.0669,  0.1317,
        #  0.1193,  0.1303,  0.1309,  0.0113,  0.0635,  0.0593,  0.0245,  0.1277,
        #  0.1327,  0.0865,  0.0953,  0.1302,  0.1142, -0.6170,  0.0630, -0.6613,
        #  0.0278,  0.1352,  0.0917,  0.1272,  0.1061,  0.1045,  0.1336,  0.0073,
        #  0.1125,  0.0359,  0.0463,  0.0147,  0.1349,  0.0523,  0.1349,  0.1310,
        # -0.1813,  0.1307,  0.0495,  0.0503,  0.1023,  0.1320,  0.0452,  0.0000,
        #  0.0029,  0.1113,  0.0868,  0.1178,  0.1080,  0.0638,  0.1339,  0.1342,
        #  0.0461,  0.0482,  0.1349,  0.0283,  0.1231,  0.0375,  0.1345,  0.0882,
        #  0.0711,  0.0013,  0.1162,  0.1281,  0.0045,  0.1148,  0.0997,  0.1019,
        #  0.0419,  0.0106,  0.0211,  0.1332,  0.1196,  0.1165,  0.0487,  0.1357,
        #  0.0646,  0.1337,  0.0315,  0.1301,  0.0658, -0.1251,  0.0306,  0.0525,
        #  0.0622,  0.1254,  0.1297,  0.0731,  0.1324,  0.1091,  0.0725,  0.0372,
        #  0.0512,  0.0461,  0.1248,  0.0673, -0.0837,  0.0664,  0.0877,  0.0626]
        # [0.2840,  0.0823,  0.0029,  0.0766,  0.0819,  0.1371,  0.0730,  0.0686,
        #  0.0448,  0.3122,  0.0238,  0.0308,  0.0947,  0.2315,  0.0556,  0.0747,
        #  0.0628,  0.0270,  0.0709,  0.0180,  0.0803,  0.1077,  0.0309,  0.0786,
        #  0.0817,  0.0371,  0.0136,  0.1261,  0.0741,  0.0338,  0.0117,  0.1532,
        #  0.0711,  0.2741,  0.0785,  0.0806,  0.0821,  0.1312,  0.0630,  0.0567,
        #  0.0537,  0.0693,  0.1066,  0.2561,  0.0227,  0.0802,  0.0587,  0.0487,
        #  0.1302,  0.0218,  0.3879,  0.0543,  0.0527,  0.1776,  0.1057,  0.0263,
        #  0.1186, -0.0023,  0.0477,  0.0629,  0.0611,  0.0499,  0.0231,  0.0045,
        #  0.0320,  0.0692,  0.0782,  0.2990,  0.0181,  0.0435,  0.0389,  0.3800,
        #  0.0730,  0.0553,  0.2864,  0.0773,  0.0464,  0.1532,  0.0693,  0.0461,
        #  0.0446,  0.0923,  0.0813,  0.0429,  0.1529,  0.0791,  0.0803,  0.0813,
        #  0.0297,  0.2888,  0.0412,  0.0710,  0.1928,  0.0005,  0.1055,  0.4266,
        #  0.0144,  0.0808,  0.0619,  0.0123,  0.0467,  0.0177,  0.0450,  0.2580,
        #  0.1835,  0.0662,  0.0298,  0.1451,  0.0734,  0.0822,  0.0449,  0.0712,
        #  0.0125,  0.0728,  0.0092,  0.0807,  0.2655,  0.2286,  0.0099,  0.0746,
        #  0.0208,  0.0175,  0.1132,  0.0440,  0.0362,  0.4518,  0.0794,  0.1405]

# loss = (np.array(loss) - np.amin(loss))/(np.amax(loss) - np.amin(loss))
# loss = loss*np.pi
# l_tau_tmp = torch.tensor(l_tau)
# print('l-tau: min:{}, max:{}'.format(l_tau_tmp.min(), l_tau_tmp.max()))
# l_tau_norm = (np.array(l_tau) - np.amin(l_tau))/(np.amax(l_tau) - np.amin(l_tau))
#
# y = 0.5*np.cos([(a+1./300)*np.pi for a in loss_norm])+0.5
# std_d_ = [a*b for a,b in zip(filter, l_tau_norm)]
# filter_2 = [a*b for a,b in zip(filter, filter)]
# # plt.scatter(loss, y, alpha=0.6, label='y')
# plt.scatter(loss, filter_2, label='filter*2')
# plt.scatter(loss, filter, alpha=0.6, label='filter')
# #plt.scatter(loss, loss_norm, alpha=0.6, label='loss_norm')
#
# # plt.scatter(loss, l_tau, alpha=0.6, label='|l-tau|')
# plt.scatter(loss, l_tau_norm, alpha=0.6, label='|l-tau|-norm')
# # plt.scatter(loss, std_d, alpha=0.6, label='std_d=filter*|l-tau|')
# plt.scatter(loss, std_d_, alpha=0.6,  label="std_d'=filter*|l-tau|-nrom")
# plt.scatter(loss, [a*b for a,b in zip(std_d_, std_d_)], alpha=0.6, label="var_d'=std_d'*std_d'")
# plt.title('Interesting Graph')
# plt.legend()
# plt.grid(True)
# plt.show()

# tensor([ 0.1667,  0.7458,  0.3100,  0.5927,  0.4952,  0.5003,  0.3576, -1.0882,
#          0.0520,  0.9165], device='cuda:0')
# tensor([[ 0.1667,  0.7458,  0.3100,  ..., -1.0882,  0.0520,  0.9165],
#         [ 1.1013,  0.3326,  0.2399,  ..., -0.2348, -0.3245, -0.5042],
#         [ 0.2184,  1.3964, -0.8037,  ...,  0.6934,  0.1149,  0.3326],
#         ...,
#         [-0.0786, -0.0151,  0.2385,  ..., -0.0417, -0.0379,  0.2707],
#         [ 1.0181,  0.0170,  0.2811,  ..., -0.6695, -0.5117,  0.1046],
#         [-0.5276,  0.4022, -0.0740,  ..., -0.4797,  0.8482,  0.7630]],
#        device='cuda:0')

def plot_noise_var(loss, filter, l_tau, l_tau_norm, std_d_, step, epoch, args):
    filter_2 = [a * b for a, b in zip(filter, filter)]
    plt.scatter(loss, filter_2, label='filter*2')
    plt.scatter(loss, filter, alpha=0.6, label='filter')
    # plt.scatter(loss, loss_norm, alpha=0.6, label='loss_norm')

    plt.scatter(loss, l_tau, alpha=0.6, label='|l-tau|')
    plt.scatter(loss, l_tau_norm, alpha=0.6, label='|l-tau|-norm')
    # plt.scatter(loss, std_d, alpha=0.6, label='std_d=filter*|l-tau|')
    plt.scatter(loss, std_d_, alpha=0.6, label="std_d'=filter*|l-tau|-nrom")
    plt.scatter(loss, [a * b for a, b in zip(std_d_, std_d_)], alpha=0.6, label="var_d'=std_d'*std_d'")
    plt.title('Epoch{}-Step{}-{}'.format(epoch, step, args.loss_type))
    plt.legend()
    plt.grid(True)
    plt.show()